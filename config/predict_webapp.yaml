test: True
task: classification
dataset:
  # dir: /home/simon/datasets/galaxy_zoo/
  dir: ./gzoo/interface/inference_webapp/
  name:
  images: images_test_rev1/
  train_labels: classification_labels_train_val.csv
  test_labels: classification_labels_test.csv
  predictions: predictions/training_solutions_rev1.csv
use_cuda: False
seed: 0          # seed for initializing training.
workers: 4       # number of data loading workers
batch_size: 256  # mini-batch size, this is the total
      # batch size of all GPUs on the current node when
      # using Data Parallel or Distributed Data Parallel
print_freq: 10
model:
  arch: 'resnet18'
  path: 'models/resnet18.pth.tar'  # path to model
  output_constraints: True
  pretrained: True
ensembling:
  enable: False
  n_estimators: 50
template: "all_ones_benchmark.csv"
output: "predictions/predictions.csv"
world_size: -1     # number of nodes for distributed training
rank: -1      # node rank for distributed training
dist_url: tcp://224.66.41.62:23456  # url used to set up distributed training
dist_backend: nccl  # distributed backend
gpu: null     # GPU id to use.
multiprocessing_distributed: False  # Use multi-processing distributed training to launch
                        #  N processes per node, which has N GPUs. This is the
                        #  fastest way to use PyTorch for either single node or
                        #  multi node data parallel training

